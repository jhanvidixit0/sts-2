{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkHk_qu9-574"
      },
      "outputs": [],
      "source": [
        "Q1)  Explain the properties of the F-distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS ) The F-distribution is a continuous probability distribution often used in analysis of variance (ANOVA), regression analysis, and hypothesis testing, especially in comparing variances. Here are the main properties of the F-distribution:\n",
        "\n",
        "1. Shape and Skewness\n",
        "\n",
        "The F-distribution is positively skewed, meaning it is skewed to the right.\n",
        "The skewness decreases as the degrees of freedom (for both the numerator and denominator) increase, making the distribution more symmetric with larger samples.\n",
        "2. Degrees of Freedom (df)\n",
        "\n",
        "The shape of the F-distribution depends on two degrees of freedom parameters:\n",
        "\n",
        "ùëë1\n",
        "\n",
        "  (numerator degrees of freedom) and\n",
        "\n",
        "ùëë2\n",
        "  (denominator degrees of freedom).\n",
        "These parameters come from the variances of two independent chi-square distributions.\n",
        "\n",
        "3. Non-Negativity\n",
        "\n",
        "The F-distribution only takes on values greater than or equal to zero. This is because it represents a ratio of two variances, which are always non-negative.\n",
        "\n",
        "4. Mean of the F-Distribution\n",
        "\n",
        "The mean of the F-distribution is\n",
        "ùëë\n",
        "2\n",
        "ùëë\n",
        "2\n",
        "‚àí\n",
        "2\n",
        "d\n",
        "2\n",
        "‚Äã\n",
        " ‚àí2\n",
        "d\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "  for\n",
        "ùëë\n",
        "2\n",
        ">\n",
        "\n",
        " >2.\n",
        "If\n",
        "ùëë\n",
        "2\n",
        "‚â§\n",
        "2\n",
        "d\n",
        "2\n",
        "‚Äã\n",
        " ‚â§2, the mean is undefined.\n",
        "\n",
        "6. Asymptotic Behavior\n",
        "As both\n",
        "ùëë\n",
        "1\n",
        "d\n",
        "1\n",
        "‚Äã\n",
        "  and\n",
        "ùëë\n",
        "2\n",
        "d\n",
        "2\n",
        "‚Äã\n",
        "  increase, the F-distribution approaches a normal distribution.\n",
        "For large values of\n",
        "ùëë\n",
        "1\n",
        "d\n",
        "1\n",
        "‚Äã\n",
        "  and\n",
        "ùëë\n",
        "2\n",
        "d\n",
        "2\n",
        "‚Äã\n",
        " , the F-distribution can be approximated by a chi-square distribution ratio or the normal distribution, especially if\n",
        "ùëë\n",
        "1\n",
        "d\n",
        "1\n",
        "‚Äã\n",
        "  and\n",
        "ùëë\n",
        "2\n",
        "d\n",
        "2\n",
        "‚Äã\n",
        "  are large.\n",
        "\n",
        "7. Applications\n",
        "The F-distribution is frequently used in hypothesis tests to compare variances, such as in ANOVA (to test whether multiple population means are equal).\n",
        "It‚Äôs also applied in regression analysis to test the overall significance of a model.\n",
        "The F-distribution is thus key in comparing the variability in different datasets and for making inferences about population variances.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fHVCR212_5Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q2)  In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "0MP8JeRZBJZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans) The F-distribution is widely used in statistical tests where the goal is to compare variances or test for overall effects in regression models. Here are the main types of statistical tests that use the F-distribution and why it is particularly suitable for each:\n",
        "\n",
        "1. Analysis of Variance (ANOVA)\n",
        "Purpose: ANOVA tests whether there are statistically significant differences between the means of three or more independent groups.\n",
        "Why the F-Distribution?: In ANOVA, the F-statistic is the ratio of the variance between group means (explained variance) to the variance within the groups (unexplained variance). The F-distribution is suitable here because it models the ratio of two independent chi-squared distributions, representing these two variances. This makes it ideal for detecting differences in means by evaluating how much of the total variance can be attributed to group differences.\n",
        "\n",
        "2. Regression Analysis Purpose: In multiple regression, the F-test is used to assess the overall significance of a model by testing whether all regression coefficients (except the intercept) are equal to zero.\n",
        "Why the F-Distribution?: The F-distribution provides a framework for comparing the variance explained by the regression model to the variance left unexplained. This helps determine whether the independent variables collectively explain a significant amount of variance in the dependent variable.\n",
        "\n",
        "3. Testing for Equality of Variances (e.g., Levene‚Äôs Test)\n",
        "Purpose: Levene‚Äôs Test and other similar tests use the F-distribution to determine if multiple groups have equal variances, which is a common assumption in ANOVA and t-tests.\n",
        "Why the F-Distribution?: Here, the F-statistic is calculated as a ratio of variances. Since the F-distribution is designed to handle variance ratios, it provides an appropriate distributional basis for determining whether observed variance differences are statistically significant.\n",
        "\n",
        "4. Comparing Two Variances (F-Test for Variances)\n",
        "Purpose: The F-test can directly compare the variances of two independent samples to determine if they are significantly different.\n",
        "Why the F-Distribution?: The F-distribution is inherently designed for testing ratios of variances, making it a natural fit for testing whether two variances differ, as it allows for the calculation of critical values based on the ratio of these variances.\n",
        "The F-distribution's structure, particularly in modeling ratios of variances, makes it ideal for these types of tests, where the aim is often to determine if variability between or within groups is greater than expected by chance alone."
      ],
      "metadata": {
        "id": "op4n9SGyBTiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " Q 3) What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?"
      ],
      "metadata": {
        "id": "deWh-bRsB2xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans) To conduct an F-test for comparing the variances of two populations, several key assumptions must be met to ensure the validity of the test results. These assumptions are:\n",
        "\n",
        "1. Independence of Samples\n",
        "The two samples being compared must be independently selected from their respective populations. This means that the observations in one sample should not influence the observations in the other sample.\n",
        "2. Normality of Populations\n",
        "Both populations from which the samples are drawn should follow a normal distribution. The F-test is sensitive to deviations from normality; if the populations are not normal, the F-test may produce misleading results, especially with smaller sample sizes.\n",
        "3. Random Sampling\n",
        "The samples should be randomly selected from the populations. Random sampling ensures that the samples are representative of the populations, which is crucial for making valid inferences from the test.\n",
        "4. Ratio of Variances\n",
        "The F-test is based on the ratio of the sample variances,\n",
        "ùëÜ\n",
        "1\n",
        "2\n",
        "/\n",
        "ùëÜ\n",
        "2\n",
        "2\n",
        "S\n",
        "1\n",
        "2\n",
        "‚Äã\n",
        " /S\n",
        "2\n",
        "2\n",
        "‚Äã\n",
        " . By convention, the larger variance should be placed in the numerator to make the F-statistic greater than or equal to 1. This practice is not an assumption but helps interpret the F-statistic with standard critical values.\n",
        "5. Independent Variances\n",
        "The variances in each population should be independent. This means that the variability in one population should not affect the variability in the other. This is particularly important when samples are taken from different groups or conditions.\n",
        "If any of these assumptions are violated, particularly normality or independence, the F-test results may be unreliable. In such cases, alternative tests, such as Levene‚Äôs test or the Brown-Forsythe test, which are less sensitive to violations of normality, may be more appropriate for comparing variances.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Egr2Y6iJB91h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4) What is the purpose of ANOVA, and how does it differ from a t-test."
      ],
      "metadata": {
        "id": "sYKQwouwCDUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans) The purpose of Analysis of Variance (ANOVA) is to determine whether there are statistically significant differences among the means of three or more independent groups. It is widely used in experiments and surveys to compare group means and test hypotheses about group differences. Here‚Äôs how ANOVA works and how it differs from the t-test:\n",
        "\n",
        "Purpose of ANOVA\n",
        "Testing Multiple Group Means: ANOVA tests whether at least one group mean is significantly different from others. Instead of comparing two groups at a time, as with a t-test, ANOVA assesses differences across multiple groups simultaneously.\n",
        "\n",
        "Partitioning Variance: ANOVA partitions the total variance observed in the data into \"between-group variance\" (differences among group means) and \"within-group variance\" (variability within each group). It then evaluates whether the between-group variance is large enough relative to the within-group variance to conclude that there are meaningful differences among the group means.\n",
        "\n",
        "Hypothesis Testing:\n",
        "\n",
        "The null hypothesis in ANOVA states that all group means are equal (i.e., no significant difference exists between them).\n",
        "The alternative hypothesis states that at least one group mean differs from the others.\n",
        "Use in Experimental Design: ANOVA is essential in factorial experiments where more than one independent variable is involved. It allows researchers to evaluate the main effects of each independent variable and any interaction effects between them.\n",
        "\n",
        "Why Use ANOVA Over Multiple t-Tests?\n",
        "\n",
        "Error Inflation: Conducting multiple t-tests to compare each pair of groups increases the risk of Type I errors (false positives). ANOVA controls for this risk by testing all groups simultaneously.\n",
        "\n",
        "Efficiency: ANOVA is more efficient when dealing with three or more groups, as it avoids redundancy and the need for numerous pairwise comparisons.\n",
        "\n",
        "When to Use ANOVA and t-Test\n",
        "\n",
        "Use a t-test when you are comparing the means of two groups only.\n",
        "Use ANOVA when you have three or more groups to compare or need to evaluate interactions in multi-factorial designs.\n",
        "In cases where ANOVA indicates significant differences among groups, post hoc tests (such as Tukey‚Äôs HSD) are often used to determine specifically which group means differ from each other.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p0e1xuM-DQwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5)  Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups."
      ],
      "metadata": {
        "id": "-KGJTHKGDm1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans) A one-way ANOVA is preferred over multiple t-tests when comparing more than two groups because it provides a more efficient, accurate, and statistically reliable way to assess whether there are significant differences among multiple group means. Here‚Äôs when and why you would use a one-way ANOVA instead of multiple t-tests:\n",
        "\n",
        "When to Use a One-Way ANOVA\n",
        "Comparing Three or More Groups: A one-way ANOVA is suitable when you have one independent variable with three or more levels (e.g., different treatment conditions, age groups, or categories).\n",
        "Single Independent Variable: Use a one-way ANOVA if you are analyzing the effect of a single factor (independent variable) on a dependent variable and want to compare the means across multiple groups within that factor.\n",
        "Why Use One-Way ANOVA Instead of Multiple t-Tests\n",
        "Type I Error Control (False Positives):\n",
        "\n",
        "Each time a t-test is conducted, there is a risk of committing a Type I error (incorrectly rejecting the null hypothesis). When conducting multiple t-tests on the same data, this error probability compounds.\n",
        "For instance, if you conduct three t-tests at a 5% significance level, the cumulative Type I error rate is actually higher than 5%, increasing the chance of finding a significant result simply by chance.\n",
        "A one-way ANOVA controls the overall Type I error rate by conducting a single test across all groups, thus avoiding the inflation of error rates.\n",
        "Efficiency and Simplicity:\n",
        "\n",
        "A one-way ANOVA provides a single F-test that evaluates whether there is a significant difference among all group means at once. This approach is simpler and more efficient than performing multiple t-tests, especially with a large number of groups.\n",
        "Running multiple t-tests is time-consuming and complex, as it requires numerous pairwise comparisons, whereas a one-way ANOVA summarizes the data into one test statistic (the F-statistic).\n",
        "Interpretability:\n",
        "\n",
        "The one-way ANOVA provides a clear, overall answer to the question of whether there are differences among the group means without needing individual comparisons for each pair.\n",
        "If the one-way ANOVA results indicate significant differences, post hoc tests (such as Tukey‚Äôs HSD) can be conducted to determine exactly which group means differ, without increasing the overall Type I error rate.\n",
        "Assumptions:\n",
        "\n",
        "Both t-tests and ANOVA rely on similar assumptions: normality of the populations, homogeneity of variances, and independence of observations.\n",
        "However, ANOVA is designed to handle comparisons among multiple groups, making it more robust for multiple-group comparisons.\n",
        "Example Scenario\n",
        "Suppose a researcher is testing the effect of three different diets (Diet A, Diet B, and Diet C) on weight loss in three different groups of participants. Using multiple t-tests, the researcher would need to conduct three pairwise comparisons: (Diet A vs. Diet B), (Diet A vs. Diet C), and (Diet B vs. Diet C). Each comparison would increase the risk of a Type I error. Instead, a one-way ANOVA allows the researcher to test all three diets simultaneously, giving a single F-statistic and controlling the error rate.\n",
        "\n",
        "Summary\n",
        "In conclusion, a one-way ANOVA is preferable when comparing more than two groups because it controls for Type I error inflation, is more efficient, and offers clearer interpretability than multiple t-tests. After finding a significant result with ANOVA, post hoc tests can be used for further investigation, if needed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bRj_4c1kDtzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6) . Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?"
      ],
      "metadata": {
        "id": "AFxszSyiD3-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans) In Analysis of Variance (ANOVA), the total variance observed in the data is partitioned into two main components: between-group variance and within-group variance. This partitioning allows ANOVA to determine whether differences among group means are statistically significant. Here‚Äôs how the variance is partitioned and how it contributes to the calculation of the F-statistic.\n",
        "1. Total Variance (Total Sum of Squares, SST)\n",
        "Total variance represents the overall variability in the data across all groups. In ANOVA, this is calculated as the total sum of squares (SST), which measures the total deviation of individual observations from the grand mean (the mean of all data points across groups).\n",
        "\n",
        "Mathematically, the Total Sum of Squares (SST) is\n",
        "SST=\n",
        "i=1\n",
        "‚àë\n",
        "G\n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "i\n",
        " (X\n",
        "ij\n",
        "‚Äã\n",
        " ‚àí\n",
        "Xgrand\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "where\n",
        "ùê∫\n",
        "G is the number of groups,\n",
        "ùëõ\n",
        "ùëñ\n",
        "n\n",
        "i\n",
        "‚Äã\n",
        "  is the number of observations in each group,\n",
        "ùëã\n",
        "ùëñ\n",
        "ùëó\n",
        "X\n",
        "ij\n",
        "‚Äã\n",
        "  is an individual observation, and\n",
        "ùëã\n",
        "‚Äæ\n",
        "grand\n",
        "X\n",
        "  \n",
        "grand\n",
        "‚Äã\n",
        "  is the overall mean of all observations.\n",
        "\n",
        "In Analysis of Variance (ANOVA), the total variance observed in the data is partitioned into two main components: between-group variance and within-group variance. This partitioning allows ANOVA to determine whether differences among group means are statistically significant. Here‚Äôs how the variance is partitioned and how it contributes to the calculation of the F-statistic.\n",
        "\n",
        "1. Total Variance (Total Sum of Squares, SST)\n",
        "Total variance represents the overall variability in the data across all groups. In ANOVA, this is calculated as the total sum of squares (SST), which measures the total deviation of individual observations from the grand mean (the mean of all data points across groups).\n",
        "Mathematically, the Total Sum of Squares (SST) is:\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùëá\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùê∫\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùëñ\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "ùëó\n",
        "‚àí\n",
        "ùëã\n",
        "‚Äæ\n",
        "grand\n",
        ")\n",
        "2\n",
        "SST=\n",
        "i=1\n",
        "‚àë\n",
        "G\n",
        "‚Äã\n",
        "  \n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        " (X\n",
        "ij\n",
        "‚Äã\n",
        " ‚àí\n",
        "X\n",
        "  \n",
        "grand\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "where\n",
        "ùê∫\n",
        "G is the number of groups,\n",
        "ùëõ\n",
        "ùëñ\n",
        "n\n",
        "i\n",
        "‚Äã\n",
        "  is the number of observations in each group,\n",
        "ùëã\n",
        "ùëñ\n",
        "ùëó\n",
        "X\n",
        "ij\n",
        "‚Äã\n",
        "  is an individual observation, and\n",
        "ùëã\n",
        "‚Äæ\n",
        "grand\n",
        "X\n",
        "  \n",
        "grand\n",
        "‚Äã\n",
        "  is the overall mean of all observations.\n",
        "2. Between-Group Variance (Between-Group Sum of Squares, SSB)\n",
        "Between-group variance (or between-group sum of squares, SSB) measures the variability due to differences among the group means. It indicates how much the group means deviate from the grand mean.\n",
        "If there are large differences between group means, the between-group variance will be high, suggesting that the group means are distinct from each other.\n",
        "\n",
        "The Between-Group Sum of Squares (SSB) is calculated as:\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùê∫\n",
        "ùëõ\n",
        "ùëñ\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëã\n",
        "‚Äæ\n",
        "grand\n",
        ")\n",
        "2\n",
        "SSB=\n",
        "i=1\n",
        "‚àë\n",
        "G\n",
        " n\n",
        "i\n",
        " (\n",
        "X\n",
        "i\n",
        "X\n",
        "  \n",
        "grand\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "where\n",
        "ùëã\n",
        "‚Äæ\n",
        "ùëñ\n",
        "X i is the mean of group\n",
        "ùëñ\n",
        "i, and\n",
        "ùëã\n",
        "grand\n",
        "X\n",
        "  is the grand mean.\n",
        "\n",
        "  Summary\n",
        "  \n",
        "This partitioning of variance into between-group and within-group components allows ANOVA to compare the group means efficiently. The F-statistic, based on this partitioning, helps determine whether the observed differences among group means are statistically significant. If the F-statistic is significantly large, it indicates that the between-group variance is much greater than the within-group variance, suggesting that at least one group mean differs significantly from the others."
      ],
      "metadata": {
        "id": "7-A8fWAQD-w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q7) Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
      ],
      "metadata": {
        "id": "dGpqbYIzE7-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS) The classical (frequentist) approach to ANOVA and the Bayesian approach both seek to determine if there are significant differences among group means. However, they differ fundamentally in how they handle uncertainty, parameter estimation, and hypothesis testing. Here‚Äôs a comparison of these two approaches:\n",
        "\n",
        "1. Handling of Uncertainty\n",
        "Frequentist ANOVA:\n",
        "\n",
        "In the frequentist approach, uncertainty is addressed through the concept of sampling distributions and p-values. Uncertainty is represented by the probability of observing an effect as extreme as, or more extreme than, the one in the data, assuming the null hypothesis is true.\n",
        "Confidence intervals and p-values quantify uncertainty around parameter estimates, but they are interpreted as long-run frequencies (i.e., across repeated sampling from the population).\n",
        "Bayesian ANOVA:\n",
        "\n",
        "The Bayesian approach explicitly models uncertainty by using probability distributions for parameters. Bayesian ANOVA represents uncertainty about parameters as probability distributions (posterior distributions) after observing the data.\n",
        "Bayesian ANOVA provides credible intervals (e.g., a 95% credible interval) that represent the probability that the parameter lies within a specified range, given the observed data and prior beliefs.\n",
        "2. Parameter Estimation\n",
        "Frequentist ANOVA:\n",
        "\n",
        "The frequentist approach estimates group means and variances as point estimates based on the observed sample data. For instance, mean square between (MSB) and mean square within (MSW) are computed directly from the data to estimate the variance components.\n",
        "These estimates rely solely on the sample data, without incorporating any prior information. The frequentist approach treats parameters as fixed, unknown values and does not quantify their uncertainty directly in terms of probability distributions.\n",
        "Bayesian ANOVA:\n",
        "\n",
        "In Bayesian ANOVA, parameters (such as group means and variances) are estimated as probability distributions rather than point estimates. The process combines prior distributions (reflecting previous knowledge or beliefs about parameters) with the likelihood of observed data to produce a posterior distribution.\n",
        "This posterior distribution reflects both the prior information and the observed data, allowing for a richer representation of parameter uncertainty. For example, if prior information suggests a particular range of means, this can influence the final estimates after observing data.\n",
        "3. Hypothesis Testing\n",
        "Frequentist ANOVA:\n",
        "\n",
        "Hypothesis testing in frequentist ANOVA uses p-values to determine if there are significant differences among group means. The null hypothesis (that all group means are equal) is tested using the F-statistic.\n",
        "A small p-value (typically below a chosen significance level like 0.05) suggests that the null hypothesis can be rejected, implying at least one group mean differs from others. However, it doesn‚Äôt quantify the probability of the null hypothesis itself, only the probability of the data under the assumption that the null hypothesis is true.\n",
        "Bayesian ANOVA:\n",
        "\n",
        "In Bayesian ANOVA, hypothesis testing is approached through posterior probabilities. Rather than rejecting or failing to reject a null hypothesis, Bayesian methods estimate the probability of different hypotheses or parameter values given the observed data.\n",
        "For instance, Bayesian ANOVA might provide the probability that each group mean differs from the others. It could also offer Bayes factors to compare models (e.g., a model assuming all means are equal vs. one assuming differences among means), giving a direct measure of the evidence in favor of one model over another.\n",
        "4. Interpretation of Results\n",
        "Frequentist ANOVA:\n",
        "\n",
        "In the frequentist approach, interpretations rely on the assumption of repeated sampling. For example, a 95% confidence interval means that if the experiment were repeated many times, 95% of the calculated intervals would contain the true parameter value.\n",
        "The conclusions are often based on binary decisions (reject or fail to reject the null hypothesis) without quantifying the probability of hypotheses.\n",
        "Bayesian ANOVA:\n",
        "\n",
        "Bayesian results are interpreted in terms of probability distributions directly related to the observed data. For example, a 95% credible interval means there is a 95% probability that the parameter lies within this interval given the data and prior information.\n",
        "The Bayesian approach allows for a more nuanced interpretation, such as estimating the probability that each group mean is larger than another or calculating the probability of certain effect sizes.\n",
        "5. Role of Prior Information\n",
        "Frequentist ANOVA:\n",
        "The frequentist approach does not incorporate prior information; it relies solely on the data collected in the current experiment. Each study is treated as independent of any prior knowledge.\n",
        "Bayesian ANOVA:\n",
        "Bayesian ANOVA incorporates prior information through prior distributions, which can be informed by previous studies, expert knowledge, or beliefs about the parameter values. Priors can be non-informative (vague, to allow the data to dominate) or informative (based on strong prior knowledge).\n",
        "The choice of prior can affect the results, especially with small sample sizes. This flexibility is advantageous when relevant prior knowledge is available but requires careful consideration to avoid bias.\n"
      ],
      "metadata": {
        "id": "YIVLr5LYFxhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8) Question: You have two sets of data representing the incomes of two different professions1\n",
        "V Profession A: [48, 52, 55, 60, 62'\n",
        "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "hzLuEvXhF_AD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-statistic for comparing the variances of incomes between Profession A and Profession B is approximately 2.09, with a p-value of 0.75.\n",
        "\n",
        "Interpretation\n",
        "Since the p-value (0.75) is much higher than a typical significance level (e.g., 0.05), we fail to reject the null hypothesis that the variances of the two professions' incomes are equal. This indicates that there is no statistically significant difference in the income variances between Profession A and Profession B based on the F-test."
      ],
      "metadata": {
        "id": "K2YHQrKyHAMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Income data for each profession\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate variances of both groups\n",
        "var_A = np.var(profession_A, ddof=1)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "\n",
        "# F-statistic (larger variance in the numerator to ensure F >= 1)\n",
        "F_statistic = var_A / var_B if var_A >= var_B else var_B / var_A\n",
        "\n",
        "# Degrees of freedom\n",
        "df1 = len(profession_A) - 1\n",
        "df2 = len(profession_B) - 1\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = stats.f.cdf(F_statistic, df1, df2) if var_A >= var_B else 1 - stats.f.cdf(F_statistic, df1, df2)\n",
        "\n",
        "F_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "171fIzV3Gwfk",
        "outputId": "8bf28abc-12ef-4689-c6fd-da9b6cda41fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.089171974522293, 0.7534757004973305)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 9)  Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        "V Region A: [160, 162, 165, 158, 164'\n",
        "V Region B: [172, 175, 170, 168, 174'\n",
        "V Region C: [180, 182, 179, 185, 183'\n",
        "V Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value ."
      ],
      "metadata": {
        "id": "3FmNxhzoHF3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heights data for each region\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "f_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuuLbSb_HPlo",
        "outputId": "7627390d-eefd-41a6-eb4d-6b69746eca9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The one-way ANOVA for comparing the heights across three regions yields an F-statistic of approximately 67.87 and a p-value of 2.87 x 10^-7.\n",
        "\n",
        "Interpretation\n",
        "Since the p-value is extremely small (much lower than a typical significance level, such as 0.05), we reject the null hypothesis. This indicates that there are statistically significant differences in the average heights between the three regions."
      ],
      "metadata": {
        "id": "ClwV0NNRHcCf"
      }
    }
  ]
}